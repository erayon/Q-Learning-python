{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Example by Code python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# source: http://mnemstudio.org/path-finding-q-learning-tutorial.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import all dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as random\n",
    "from skimage import io\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initilize matrix Q as a zero matirx\n",
    "Q=np.zeros(36).reshape(6,6)\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a Reward State and Action state corresponding to rows \n",
    "#and columns representation Action of that state change\n",
    "\n",
    "R=np.zeros(36).reshape(6,6)\n",
    "# first make all value 0 by default and change to -1 all of them\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        R[i][j]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -1.,   -1.,   -1.,   -1.,    0.,   -1.],\n",
       "       [  -1.,   -1.,   -1.,    0.,   -1.,  100.],\n",
       "       [  -1.,   -1.,   -1.,    0.,   -1.,   -1.],\n",
       "       [  -1.,    0.,    0.,   -1.,    0.,   -1.],\n",
       "       [   0.,   -1.,   -1.,    0.,   -1.,  100.],\n",
       "       [  -1.,    0.,   -1.,   -1.,    0.,  100.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now put some reward\n",
    "R[0,4]=0\n",
    "R[1,3]=0\n",
    "R[1,5]=100\n",
    "R[2,3]=0\n",
    "R[3,1]=0\n",
    "R[3,2]=0\n",
    "R[3,4]=0\n",
    "R[4,0]=0\n",
    "R[4,3]=0\n",
    "R[4,5]=100\n",
    "R[5,1]=0\n",
    "R[5,4]=0\n",
    "R[5,5]=100\n",
    "\n",
    "# final Reward Matrix\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set a hyper parameter gamma\n",
    "gamma=0.8 # hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Q_state(Q,R,gamma,state):\n",
    "    possible_actions=[]\n",
    "    for i in range(6):\n",
    "        if(R[state,i])>=0:\n",
    "            possible_actions.append(i)\n",
    "    action=random.choice(possible_actions)\n",
    "    temp=[]\n",
    "    for i in range(6):\n",
    "        if(R[action,i])>=0:\n",
    "            temp.append(Q[action,i])\n",
    "    max_reward=max(temp)\n",
    "    Q[state,action]=R[state,action]+gamma*max_reward\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "episode=10000\n",
    "goal_state=5\n",
    "for i in range(episode):\n",
    "    current_state=random.randint(0,5)\n",
    "    while True:\n",
    "        next_state=Q_state(Q,R,gamma,current_state)\n",
    "        current_state=next_state\n",
    "        if(current_state==goal_state):\n",
    "             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the Q matrix with the highest value and multiply with 100\n",
    "def Q_normalize(Q):\n",
    "    max_val=np.max(Q)\n",
    "    return np.multiply((np.divide(Q,max_val)),100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0. ,    0. ,    0. ,    0. ,   80. ,    0. ],\n",
       "       [   0. ,    0. ,    0. ,   64. ,    0. ,  100. ],\n",
       "       [   0. ,    0. ,    0. ,   64. ,    0. ,    0. ],\n",
       "       [   0. ,   80. ,   51.2,    0. ,   80. ,    0. ],\n",
       "       [  64. ,    0. ,    0. ,   64. ,    0. ,  100. ],\n",
       "       [   0. ,   80. ,    0. ,    0. ,   80. ,  100. ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_normalize(Q) # this is the our agent memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Once the Q matrix gets closed enough to a state of convergence, we know our agent has learned \n",
    "# the most optimal path to the goal state. Tacing the best sequence of state is as\n",
    "# simple as following the links the highest values at each state\n",
    "\n",
    "mem=Q_normalize(Q)\n",
    "def Find_path(initial_state,goal_state):\n",
    "    path=[]\n",
    "    path.append(initial_state)\n",
    "    while True:\n",
    "        t=max(mem[initial_state])\n",
    "        c=np.where(mem[initial_state]==t)\n",
    "        x=np.squeeze(c)\n",
    "        try:\n",
    "            temp_path=x[0]\n",
    "        except (IndexError):\n",
    "            temp_path=np.max(x)\n",
    "        initial_state=temp_path\n",
    "        path.append(temp_path)\n",
    "        if(initial_state==goal):\n",
    "             break\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 1, 5]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suppose that our agent in room 2 and tries to go in room no 5\n",
    "Find_path(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
